2024-06-19 00:54:26,848 - root - INFO - Configuration loaded from config/config.yaml
2024-06-19 00:54:26,878 - root - INFO - Processing dataset: customer_churn
2024-06-19 00:54:53,997 - root - INFO - Generated code: ```python
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import logging

# Setting up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()

# Data Ingestion
logger.info("Reading data from CSV file...")
train_data = pd.read_csv('train_data.csv')
test_data = pd.read_csv('test_data.csv')

# Data Preprocessing
logger.info("Handling missing values...")
train_data.fillna(train_data.mean(), inplace=True)

logger.info("Normalizing the data...")
scaler = StandardScaler()
scaled_train_data = scaler.fit_transform(train_data.drop('Churn', axis=1))
scaled_train_data = pd.DataFrame(scaled_train_data, columns=train_data.columns[:-1])

logger.info("Splitting the data into training, validation, and test sets...")
X = scaled_train_data
y = train_data['Churn']
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)

# Implementing cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Feature and Target Identification
logger.info("Identifying features and target variable...")
features = X_train.columns
target = 'Churn'

logger.info("Determining the task type...")
# Task type is binary classification based on the target variable 'Churn'

# Model Selection
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(len(features), 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.sigmoid(self.fc3(x))
        return x

model = Net()

# Model Training
logger.info("Training the model...")
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
epochs = 20

for epoch in range(epochs):
    for train_index, val_index in kf.split(X_train):
        X_tr, X_va = X_train.values[train_index], X_train.values[val_index]
        y_tr, y_va = y_train.values[train_index], y_train.values[val_index]

        inputs = torch.tensor(X_tr, dtype=torch.float)
        labels = torch.tensor(y_tr.values, dtype=torch.float)

        model.train()

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels.view(-1, 1))
        loss.backward()
        optimizer.step()

        logger.info(f'Epoch {epoch+1}, Loss: {loss.item()}')

# Model Evaluation
logger.info("Evaluating the model...")
X_val_tensor = torch.tensor(X_val.values, dtype=torch.float)
model.eval()
val_outputs = model(X_val_tensor)
val_predictions = (val_outputs > 0.5).float()
val_accuracy = accuracy_score(y_val.values, val_predictions.detach().numpy())
logger.info(f'Validation Accuracy: {val_accuracy}')

# Hyperparameter Tuning
# Hyperparameter tuning code can be added here to achieve optimal performance

# Prediction Generation
logger.info("Generating predictions on new data...")
X_test = scaler.transform(test_data.drop('Churn', axis=1))
X_test_tensor = torch.tensor(X_test, dtype=torch.float)
test_outputs = model(X_test_tensor)
test_predictions = (test_outputs > 0.5).float()

# Result Reporting
logger.info("Model performance results:")
# Additional reporting code can be added here

# End of the code
```
2024-06-19 00:54:54,101 - root - INFO - Generated code for customer_churn:
```python
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import logging

# Setting up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()

# Data Ingestion
logger.info("Reading data from CSV file...")
train_data = pd.read_csv('train_data.csv')
test_data = pd.read_csv('test_data.csv')

# Data Preprocessing
logger.info("Handling missing values...")
train_data.fillna(train_data.mean(), inplace=True)

logger.info("Normalizing the data...")
scaler = StandardScaler()
scaled_train_data = scaler.fit_transform(train_data.drop('Churn', axis=1))
scaled_train_data = pd.DataFrame(scaled_train_data, columns=train_data.columns[:-1])

logger.info("Splitting the data into training, validation, and test sets...")
X = scaled_train_data
y = train_data['Churn']
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)

# Implementing cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Feature and Target Identification
logger.info("Identifying features and target variable...")
features = X_train.columns
target = 'Churn'

logger.info("Determining the task type...")
# Task type is binary classification based on the target variable 'Churn'

# Model Selection
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(len(features), 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.sigmoid(self.fc3(x))
        return x

model = Net()

# Model Training
logger.info("Training the model...")
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
epochs = 20

for epoch in range(epochs):
    for train_index, val_index in kf.split(X_train):
        X_tr, X_va = X_train.values[train_index], X_train.values[val_index]
        y_tr, y_va = y_train.values[train_index], y_train.values[val_index]

        inputs = torch.tensor(X_tr, dtype=torch.float)
        labels = torch.tensor(y_tr.values, dtype=torch.float)

        model.train()

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels.view(-1, 1))
        loss.backward()
        optimizer.step()

        logger.info(f'Epoch {epoch+1}, Loss: {loss.item()}')

# Model Evaluation
logger.info("Evaluating the model...")
X_val_tensor = torch.tensor(X_val.values, dtype=torch.float)
model.eval()
val_outputs = model(X_val_tensor)
val_predictions = (val_outputs > 0.5).float()
val_accuracy = accuracy_score(y_val.values, val_predictions.detach().numpy())
logger.info(f'Validation Accuracy: {val_accuracy}')

# Hyperparameter Tuning
# Hyperparameter tuning code can be added here to achieve optimal performance

# Prediction Generation
logger.info("Generating predictions on new data...")
X_test = scaler.transform(test_data.drop('Churn', axis=1))
X_test_tensor = torch.tensor(X_test, dtype=torch.float)
test_outputs = model(X_test_tensor)
test_predictions = (test_outputs > 0.5).float()

# Result Reporting
logger.info("Model performance results:")
# Additional reporting code can be added here

# End of the code
```
